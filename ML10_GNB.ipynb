{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef feature_ex(x):\\n    t = []\\n    x = np.array(x)\\n    indexAx = 0\\n    indexAy = 1\\n    indexAz = 2\\n    indexGx = 3\\n    indexGy = 4\\n    indexGz = 5\\n    indexTotalAcc = 6\\n    indexTotalGyro = 7\\n    indexRoll = 8\\n    indexPitch = 9\\n    \\n    totalAcc = getTotalAxes(x[indexAx],x[indexAy],x[indexAz])\\n    totalGyro = getTotalAxes(x[indexGx],x[indexGy],x[indexGz])\\n    roll = getRoll(x[indexAx],x[indexAz])\\n    pitch = getPitch(x[indexAy],x[indexAz])\\n\\n    processedKoalaData = np.ones((10,40))\\n\\n    for i in range(6):\\n        processedKoalaData[i] = copy.deepcopy(x[i])\\n    processedKoalaData[6] = copy.deepcopy(totalAcc)\\n    processedKoalaData[7] = copy.deepcopy(totalGyro)\\n    processedKoalaData[8] = copy.deepcopy(roll)\\n    processedKoalaData[9] = copy.deepcopy(pitch)\\n\\n\\n\\n    mean = getMean2D(processedKoalaData)\\n\\n    t.append(mean)\\n\\n\\n    return t\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## you can use sklearn\n",
    "#from sklearn.datasets import load_iris\n",
    "import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "from Function import *\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score  # 準確率\n",
    "from sklearn.decomposition import PCA\n",
    "def read_csv(filename):\n",
    "    t = []\n",
    "    csv_file = open(filename,'r')\n",
    "    i =0\n",
    "    for row in csv.reader(csv_file):\n",
    "        # if (i > 2):\n",
    "        t.append(row)\n",
    "        # i+=1\n",
    "    # print (t)\n",
    "    t = np.array(t,dtype = np.float32)\n",
    "    return t\n",
    "\n",
    "def accF (x,y):\n",
    "    acc = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == y[i]:\n",
    "            acc +=1\n",
    "    acc = acc / len(x)\n",
    "    return acc \n",
    "\n",
    "\n",
    "##################################\n",
    "## your Feature extractor\n",
    "##################################\n",
    "def feature_ex(X):\n",
    "    pca = PCA(n_components=15)\n",
    "    fit = pca.fit(X)\n",
    "    #print(fit.components_)\n",
    "    return fit.components_\n",
    "'''\n",
    "def feature_ex(x):\n",
    "    t = []\n",
    "    x = np.array(x)\n",
    "    indexAx = 0\n",
    "    indexAy = 1\n",
    "    indexAz = 2\n",
    "    indexGx = 3\n",
    "    indexGy = 4\n",
    "    indexGz = 5\n",
    "    indexTotalAcc = 6\n",
    "    indexTotalGyro = 7\n",
    "    indexRoll = 8\n",
    "    indexPitch = 9\n",
    "    \n",
    "    totalAcc = getTotalAxes(x[indexAx],x[indexAy],x[indexAz])\n",
    "    totalGyro = getTotalAxes(x[indexGx],x[indexGy],x[indexGz])\n",
    "    roll = getRoll(x[indexAx],x[indexAz])\n",
    "    pitch = getPitch(x[indexAy],x[indexAz])\n",
    "\n",
    "    processedKoalaData = np.ones((10,40))\n",
    "\n",
    "    for i in range(6):\n",
    "        processedKoalaData[i] = copy.deepcopy(x[i])\n",
    "    processedKoalaData[6] = copy.deepcopy(totalAcc)\n",
    "    processedKoalaData[7] = copy.deepcopy(totalGyro)\n",
    "    processedKoalaData[8] = copy.deepcopy(roll)\n",
    "    processedKoalaData[9] = copy.deepcopy(pitch)\n",
    "\n",
    "\n",
    "\n",
    "    mean = getMean2D(processedKoalaData)\n",
    "\n",
    "    t.append(mean)\n",
    "\n",
    "\n",
    "    return t\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = []\n",
    "label = []\n",
    "feature = []\n",
    "f = glob.glob(r'40_data/down'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([0])\n",
    "\n",
    "f = glob.glob(r'40_data/up'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([0])\n",
    "\n",
    "f = glob.glob(r'40_data/left'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([1])\n",
    "f = glob.glob(r'40_data/right'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([1])\n",
    "\n",
    "f = glob.glob(r'40_data/CW'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([2])\n",
    "\n",
    "f = glob.glob(r'40_data/CCW'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([3])\n",
    "\n",
    "\n",
    "\n",
    "f = glob.glob(r'40_data/VLR'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([4])\n",
    "\n",
    "f = glob.glob(r'40_data/VRL'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([5])\n",
    "\n",
    "\n",
    "f = glob.glob(r'40_data/non'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([6])\n",
    "\n",
    "f = glob.glob(r'40_data/CRCW'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([7])\n",
    "\n",
    "f = glob.glob(r'40_data/CRCCW'+'/*.csv')\n",
    "for i in range(len(f)):\n",
    "    t = read_csv(f[i])\n",
    "    if (len(t[0])) == 40:\n",
    "        t = feature_ex(t)\n",
    "        sensor.append(t)\n",
    "        label.extend([8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor shape is : (1096, 6, 40)\n",
      "sensor shape after is : (1096, 240)\n",
      "label shape is : (1096,)\n",
      "label is : [0 0 0 ... 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "sensor = np.array(sensor)\n",
    "print ('sensor shape is :',sensor.shape)\n",
    "\n",
    "#feature_name = [ \"x[\"+str(i)+\"]\" for i in range((sensor.shape[1]*sensor.shape[2]))]\n",
    "sensor = np.reshape(sensor,(sensor.shape[0],sensor.shape[1]*sensor.shape[2]))\n",
    "label  = np.array(label)\n",
    "\n",
    "print ('sensor shape after is :',sensor.shape)\n",
    "print ('label shape is :',label.shape)\n",
    "print ('label is :',label)\n",
    "\n",
    "#fea=[ 0 , 1,  2,  3,  4,  7,  9]\n",
    "#sensor=sensor.take(fea,axis = 1)\n",
    "\n",
    "train_X, val_X, train_Y, val_Y =train_test_split(sensor, label, test_size = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data (105, 240)\n"
     ]
    }
   ],
   "source": [
    "f = glob.glob(r'testdata'+'/*.csv')\n",
    "test_data=[None]*105\n",
    "for i in range(len(f)):\n",
    "    test = read_csv(f[i])\n",
    "    s=f[i]\n",
    "    s=s.strip('testdata/')\n",
    "    s=s.strip('.csv')\n",
    "   # print(s)\n",
    "    if (len(test[0])) == 40:\n",
    "        test = feature_ex(test)\n",
    "        test_data[int(s)-1]=test\n",
    "test_data=np.array(test_data)\n",
    "test_data = np.reshape(test_data,(test_data.shape[0],test_data.shape[1]*test_data.shape[2]))\n",
    "#test_data=test_data.take(fea,axis = 1)\n",
    "print('test_data',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636364"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM \n",
    "#make_pipeline(memory=None,steps=[('standardscaler',StandardScaler(copy=True,with_mean=True,with_std=True)),('guassiannb',GaissianNB(priors=None,var_smoothing=1e-9))])\n",
    "sc=StandardScaler()\n",
    "train_X=sc.fit_transform(train_X)\n",
    "val_X=sc.fit_transform(val_X)\n",
    "test_data=sc.fit_transform(test_data)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_X, train_Y)\n",
    "#val_pred=clf.predict(val_X)\n",
    "\n",
    "clf.score(val_X, val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=clf.predict(test_data)\n",
    "with open('predict_0324.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Id','Category'])\n",
    "    c=1\n",
    "    for row in predict:\n",
    "        #print([str(c).zfill(2)+\".csv\", row])\n",
    "        writer.writerow([str(c).zfill(2)+\".csv\", row])\n",
    "        c+=1\n",
    "      # print_leaf(classify(row, my_tree))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評估模型好壞\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(val_pred, val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "'''\n",
    "cov_matrix = np.cov(train_X.T)\n",
    "eigen_val, eigen_vec = np.linalg.eig(cov_matrix)\n",
    "\n",
    "tot = sum(eigen_val)  # 總特徵值和\n",
    "var_exp = [(i / tot) for i in sorted(eigen_val, reverse=True)]\n",
    "\n",
    "eigen_pairs = [(np.abs(eigen_val[i]), eigen_vec[:, i]) for i in range(len(eigen_val))]\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))  # 降維投影矩陣W\n",
    "# print(w)\n",
    "train_X = train_X.dot(w)\n",
    "####\n",
    "cov_matrix = np.cov(val_X.T)\n",
    "eigen_val, eigen_vec = np.linalg.eig(cov_matrix)\n",
    "\n",
    "tot = sum(eigen_val)  # 總特徵值和\n",
    "var_exp = [(i / tot) for i in sorted(eigen_val, reverse=True)]\n",
    "\n",
    "eigen_pairs = [(np.abs(eigen_val[i]), eigen_vec[:, i]) for i in range(len(eigen_val))]\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))  # 降維投影矩陣W\n",
    "# print(w)\n",
    "val_X = val_X.dot(w)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sc=StandardScaler()\n",
    "train_X=sc.fit_transform(train_X)\n",
    "val_X=sc.fit_transform(val_X)\n",
    "test_data=sc.fit_transform(test_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans\n",
    "'''\n",
    "from sklearn.preprocessing import scale\n",
    "train_X=scale(train_X)\n",
    "val_X=scale(val_X)\n",
    "test_data=scale(test_data)\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "clf=KMeans(init='k-means++',n_clusters=7,random_state=6)\n",
    "clf.fit(train_X)\n",
    "'''\n",
    "'''\n",
    "# 載入 matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 設定圖形的大小\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# 圖形標題\n",
    "fig.suptitle('Cluster Center Images', fontsize=14, fontweight='bold')\n",
    "for i in range(10):\n",
    "    # 在 2x5 的網格上繪製子圖形\n",
    "    ax = fig.add_subplot(2, 5, i + 1)\n",
    "    # 顯示圖片\n",
    "    ax.imshow(clf.cluster_centers_[i].reshape((5, 2)), cmap=plt.cm.binary)\n",
    "    # 將座標軸刻度關掉\n",
    "    plt.axis('off')\n",
    "\n",
    "# 顯示圖形\n",
    "plt.show()\n",
    "'''\n",
    "val_pred=clf.predict(val_X)\n",
    "error=0\n",
    "for i,v in enumerate(val_pred):\n",
    "    if v!=val_Y[i]:\n",
    "        error+=1\n",
    "print((float)(error)/val_pred.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
